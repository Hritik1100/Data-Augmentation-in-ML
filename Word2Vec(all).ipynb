{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFZGj1EiawQAHQjnMJ2i3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hritik1100/Data-Augmentation-in-ML/blob/main/Word2Vec(all).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1b8ZT2fRJHGR"
      },
      "source": [
        "# Only needs to be run once at the top of the notebook\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHQz61x9JHgH"
      },
      "source": [
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NloVrGwtIuFi"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import sklearn\n",
        "import spacy\n",
        "data = pd.read_csv('newfile.csv',encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YoppiS6Ix-W"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7RYkITDOgSX"
      },
      "source": [
        "!pip install git+https://github.com/casics/spiral.git\n",
        "!pip install textaugment\n",
        "!pip install contractions\n",
        "!pip install -U git+https://github.com/ray-project/tune-sklearn.git && pip install 'ray[tune]'\n",
        "!!pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO1Wch4EIyI8"
      },
      "source": [
        "from textaugment import Wordnet\n",
        "t = Wordnet()\n",
        "aug = t.augment(\"The quick brown fox jumps over the lazy dog\")\n",
        "print(aug)\n",
        "print(type(aug))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NQaCoCKIyX5"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "lem = WordNetLemmatizer()\n",
        "corpus_test = []\n",
        "stopwords = set(stopwords.words('english'))\n",
        "punc = string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9fMfqEkJAFp"
      },
      "source": [
        "corpus=[]\n",
        "import contractions\n",
        "from spiral import ronin\n",
        "for i in range(len(data)):\n",
        "  #review = re.sub(r\"http\\S+\", \"\", data['Tweets'][i])\n",
        "  #review = re.sub('[^a-zA-Z]',' ',review)\n",
        "  review = re.sub(r\"http\\S+\", \"\", data['message'][i]) # remove urls\n",
        "  review = re.sub(r'<([^>]*)>', ' ', review) # remove emojis\n",
        "  review = re.sub(r'@\\w+', ' ', review) # remove at mentions\n",
        "  review = re.sub(r'#', '', review) # remove hashtag symbol\n",
        "  review = re.sub(r'[0-9]+', ' ', review) # remove numbers\n",
        "  review = re.sub(r'[^A-Za-z0-9,?.!]+', ' ', review)\n",
        "  review = review.lower()\n",
        "  l=[]\n",
        "  for word in review.split():\n",
        "    l.append(contractions.fix(word))\n",
        "  review = ' '.join(l)\n",
        "  review = t.augment(review)\n",
        "  review = ronin.split(review)\n",
        "  review = ' '.join(review)\n",
        "  review = nltk.word_tokenize(review)\n",
        "  review = [lem.lemmatize(word) for word in review if word not in stopwords and word not in punc]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmqJbEMTqqgg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "X = np.array([nlp(d).vector for d in corpus])\n",
        "y = data['label']\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76-f2KfnSWM2"
      },
      "source": [
        "X_train_vector = X_train\n",
        "X_test_vector = X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doCEXNehSnOB"
      },
      "source": [
        "params = {'clf__learning_rate':[0.1,0.3],\n",
        "          'clf__n_estimators':[70,100]\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64HcYvDQ4xoi"
      },
      "source": [
        "TuneSearch CV\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "KIXOKZ7OSBCZ",
        "outputId": "7b724dc7-a49a-41c6-ee2a-72ed6791957e"
      },
      "source": [
        "from sklearn.metrics import precision_score,confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
        "from tune_sklearn import TuneSearchCV\n",
        "from imblearn.over_sampling import SMOTE           #Run this for Cross Validation after initializing params\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score, average = 'weighted'),\n",
        "           'recall': make_scorer(recall_score, average = 'weighted'),\n",
        "           'f1': make_scorer(f1_score, average = 'weighted')\n",
        "}\n",
        "\n",
        "pipeline = Pipeline([('Smote',SMOTE(random_state=42)),('clf',GradientBoostingClassifier())])\n",
        "tune_imba = TuneSearchCV(pipeline,params,scoring=\n",
        "                         scoring,max_iters=10,n_jobs=-1,search_optimization = 'bayesian',cv=5,verbose=1,refit='precision')\n",
        "tune_imba.fit(X_train_vector,y_train)\n",
        "y_pred = tune_imba.predict(X_test_vector)\n",
        "print('validation Precision',tune_imba.best_score_)\n",
        "print(\"Test precision score - \", precision_score(y_test, y_pred))\n",
        "print(\"Test recall score - \", recall_score(y_test, y_pred))\n",
        "print(\"Test f1 score - \", f1_score(y_test, y_pred))\n",
        "print(\"Test accuracy score - \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "tune_imba.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/7.42 GiB heap, 0.0/2.54 GiB objects<br>Result logdir: /root/ray_results/_Trainable_2021-04-13_07-52-36<br>Number of trials: 10/10 (10 TERMINATED)<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:ray.tune.tune:Total run time: 4930.34 seconds (4930.30 seconds for the tuning loop).\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation Precision 0.9179314960917162\n",
            "Test precision score -  0.8417508417508418\n",
            "Test recall score -  0.8214676889375685\n",
            "Test f1 score -  0.8314855875831487\n",
            "Test accuracy score -  0.9250308261405672\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf__learning_rate': 0.2483631611523484, 'clf__n_estimators': 86}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "YQ3BR0BDJANy",
        "outputId": "48352cc9-23e5-440e-fcd9-4323c48f5b3c",
        "collapsed": true
      },
      "source": [
        "from mlxtend.plotting import plot_learning_curves\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.data import iris_data\n",
        "from mlxtend.preprocessing import shuffle_arrays_unison\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn import svm\n",
        "\n",
        "pipeline = Pipeline([('Smote',SMOTE(random_state=42)),('clf',ExtraTreesClassifier(max_features='sqrt',min_samples_leaf= 5, n_estimators=111))])\n",
        "plot_learning_curves(X_train_vector, y_train, X_test_vector, y_test, tune_imba)\n",
        "#model_pickle = pickle.loads(save_model)plot_learning_curves(X_train_vector, y_train, X_test_vector, y_test, pipeline)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ad4cddf44547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Smote'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sqrt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplot_learning_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_imba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#model_pickle = pickle.loads(save_model)plot_learning_curves(X_train_vector, y_train, X_test_vector, y_test, pipeline)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
          ]
        }
      ]
    }
  ]
}